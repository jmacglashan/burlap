package ellisPaperCode.pfLearner;

import java.util.ArrayList;
import java.util.List;

import ellisPaperCode.burlap2Code.FVToFeatureDatabase;
import ellisPaperCode.burlap2Code.LinearFVVFA;
import ellisPaperCode.propositionalFunctionToFeatures.PFToFeaturesGenerator;
import ellisPaperCode.taxi.TaxiDomain;
import ellisPaperCode.taxi.TaxiRandomStateGenerator;
import burlap.behavior.singleagent.EpisodeAnalysis;
import burlap.behavior.singleagent.Policy;
import burlap.behavior.singleagent.auxiliary.StateReachability;
import burlap.behavior.singleagent.learning.GoalBasedRF;
import burlap.behavior.singleagent.learning.lspi.LSPI;
import burlap.behavior.singleagent.learning.lspi.SARSCollector;
import burlap.behavior.singleagent.learning.lspi.SARSData;
import burlap.behavior.singleagent.planning.commonpolicies.GreedyQPolicy;
import burlap.behavior.singleagent.planning.deterministic.TFGoalCondition;
import burlap.behavior.singleagent.vfa.FeatureDatabase;
import burlap.behavior.singleagent.vfa.StateToFeatureVectorGenerator;
import burlap.behavior.singleagent.vfa.ValueFunctionApproximation;
import burlap.behavior.singleagent.vfa.rbf.RBFFeatureDatabase;
import burlap.behavior.statehashing.DiscreteStateHashFactory;
import burlap.behavior.statehashing.StateHashFactory;
import burlap.domain.singleagent.gridworld.GridWorldDomain;
import burlap.oomdp.auxiliary.DomainGenerator;
import burlap.oomdp.auxiliary.StateGenerator;
import burlap.oomdp.core.Domain;
import burlap.oomdp.core.PropositionalFunction;
import burlap.oomdp.core.State;
import burlap.oomdp.core.TerminalFunction;
import burlap.oomdp.singleagent.Action;
import burlap.oomdp.singleagent.RewardFunction;
import burlap.oomdp.singleagent.SADomain;
import burlap.oomdp.singleagent.common.SinglePFTF;
import burlap.oomdp.singleagent.common.UniformCostRF;

public class PFLearnerTest {
	public static void printPercentageTruePFs(List<PropositionalFunction> pfs, List<State> allStates) {
		System.out.print("Printing percentage of true of learned PFs...");
		StringBuilder toPrint = new StringBuilder();
		int numStates = allStates.size();
		for (PropositionalFunction pf : pfs) {
			double percTrue = 0.0;
			for (State s : allStates) {
				if (pf.isTrue(s, "")) {
					percTrue++;
				}
			}
			percTrue /= (float) numStates;
			toPrint.append(Double.toString(percTrue)+ ",");
			System.out.print(".");
		}
		System.out.println("");
		System.out.println(toPrint.toString());
	}

	public runFourRoomsExperiments(int numIterations) {
		//GRIDWORLD
		GridWorldDomain dg = new GridWorldDomain(11,11); //11x11 grid world
		dg.setMapToFourRooms(); //four rooms layout
		dg.setProbSucceedTransitionDynamics(0.8); //stochastic transitions with 0.8 success rate
		final Domain domain = dg.generateDomain(); //generate the grid world domain

		//setup initial state
		State initialState = GridWorldDomain.getOneAgentOneLocationState(domain);
		GridWorldDomain.setAgent(initialState, 0, 0);
		GridWorldDomain.setLocation(initialState, 0, 10, 10);

		//ends when the agent reaches a location
		final TerminalFunction tf = new SinglePFTF(domain.
				getPropFunction(GridWorldDomain.PFATLOCATION)); 

		//reward function definition
		final RewardFunction rf = new GoalBasedRF(new TFGoalCondition(tf), 5., -0.1);



		//Test learning using a random rollout policy
		int numRollouts = 10;
		int policyRolloutLength = 100;
		PFLearner pfLearnerPol = new PFLearner(new Policy.RandomPolicy(d), policyRolloutLength, numRollouts);
		learnedPropFuns = pfLearnerPol.getPropFuns(d, rf, tf, initialState);
		printPercentageTruePFs(learnedPropFuns, allStates);


	}


	public static void main(String[] args) {
		//TAXI
		//		DomainGenerator dg = new TaxiDomain();
		//		State initialState = TaxiDomain.getClassicState(d);
		//		TerminalFunction tf = new TaxiDomain.TaxiTF();
		//		RewardFunction rf = new UniformCostRF();

		//GRIDWORLD
		GridWorldDomain dg = new GridWorldDomain(11,11); //11x11 grid world
		dg.setMapToFourRooms(); //four rooms layout
		dg.setProbSucceedTransitionDynamics(0.8); //stochastic transitions with 0.8 success rate
		final Domain domain = dg.generateDomain(); //generate the grid world domain

		//setup initial state
		State initialState = GridWorldDomain.getOneAgentOneLocationState(domain);
		GridWorldDomain.setAgent(initialState, 0, 0);
		GridWorldDomain.setLocation(initialState, 0, 10, 10);

		//ends when the agent reaches a location
		final TerminalFunction tf = new SinglePFTF(domain.
				getPropFunction(GridWorldDomain.PFATLOCATION)); 

		//reward function definition
		final RewardFunction rf = new GoalBasedRF(new TFGoalCondition(tf), 5., -0.1);

		//COMMON CODE
		Domain d = dg.generateDomain();
		List<State> allStates = StateReachability.getReachableStates(initialState, (SADomain)d, new DiscreteStateHashFactory());
		List<PropositionalFunction> learnedPropFuns = null;

		//PF LEARNING CODE
		//Test exhaustive learning
		//		PFLearner pfLearnerEx = new PFLearner();
		//		learnedPropFuns = pfLearnerEx.getPropFuns(d, rf, tf, initialState);
		//		printPercentageTruePFs(learnedPropFuns, allStates);

		//Test learning using a random rollout policy
		int numRollouts = 10;
		int policyRolloutLength = 100;
		PFLearner pfLearnerPol = new PFLearner(new Policy.RandomPolicy(d), policyRolloutLength, numRollouts);
		learnedPropFuns = pfLearnerPol.getPropFuns(d, rf, tf, initialState);
		printPercentageTruePFs(learnedPropFuns, allStates);

		//EXPERT PFs
		//		learnedPropFuns = d.getPropFunctions();
		//		learnedPropFuns.remove(0);//remove at location for training


		System.out.println("UsedPFs: [" + learnedPropFuns.toString());
		System.out.print("Usedactions: " );
		for (Action a : d.getActions()) {
			System.out.print(a.getName() + ", ");
		}
		System.out.println("]");
		//LSPI Code
		StateToFeatureVectorGenerator featGen = new PFToFeaturesGenerator(learnedPropFuns);
		//		ValueFunctionApproximation vfApprox = new LinearFVVFA(featGen, 0.0);

		FeatureDatabase databaseConverter = new FVToFeatureDatabase(featGen, learnedPropFuns.size()); //The dimensionality is the number of features


		//Try and run LSPI with learned PFs as features

		//get a state definition earlier, we'll use it soon.

		System.out.print("Collecting S,A,R,S' for linear function approximation...");
		StateGenerator rStateGen = new TaxiRandomStateGenerator(d, initialState);
		SARSCollector collector = new SARSCollector.UniformRandomSARSCollector(d);
		SARSData dataset = collector.collectNInstances(rStateGen, rf, 10000, 1000, tf, null);

		//		for (int i = 0; i < dataset.size(); i++) {
		//			System.out.println(dataset.get(i).s);
		//		}

		System.out.println("Done collecting S, A, R, S'.");

		//Pass LSPI the PF features
		System.out.println("Creating LSPI");
		LSPI lspi = new LSPI(d, rf, tf, 0.99, databaseConverter);
		lspi.setDataset(dataset);

		System.out.println("Running PI");
		lspi.runPolicyIteration(100, 1e-6);

		GreedyQPolicy p = new GreedyQPolicy(lspi);

		Policy randPol = new Policy.RandomPolicy(d);

		int maxStepsToTake = 1000;
		EpisodeAnalysis eaRand = randPol.evaluateBehavior(initialState, rf, tf, maxStepsToTake);
		EpisodeAnalysis ea = p.evaluateBehavior(initialState, rf, tf, maxStepsToTake);
		System.out.println("ACTIONS TAKEN BY LSPI: " + ea.numTimeSteps());
		System.out.println("ACTIONS TAKEN BY RAND: " + eaRand.numTimeSteps());

	}

}
